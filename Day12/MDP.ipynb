{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c747e98",
   "metadata": {},
   "source": [
    "## What is a Markov Decision Process (MDP)?\n",
    "\n",
    "An MDP is a formal framework for decision-making where an agent interacts with an environment in discrete steps to achieve a goal.From the Reinforcement Learning (RL) perspective, an MDP is the mathematical framework that models decision-making situations where outcomes are partly random and partly under the control of an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f255cf",
   "metadata": {},
   "source": [
    "## MDP Components (RL Terms):\n",
    "States (S): All possible situations the agent can be in.\n",
    "\n",
    "Actions (A): Choices available to the agent in each state.\n",
    "\n",
    "Transition Probability (P): Probability of moving from one state to another, given an action.\n",
    "\n",
    "Reward (R): Immediate value received after a state transition.\n",
    "\n",
    "Policy (π): A strategy that defines what action to take in each state. (not considered at this moment)\n",
    "\n",
    "Discount Factor (γ): How much future rewards are valued over immediate rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331679af",
   "metadata": {},
   "source": [
    "### MDP is defined by a 5-tuple:\n",
    "               MDP = (𝑆,𝐴,𝑃,𝑅,𝛾)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9633762",
   "metadata": {},
   "source": [
    "## Example Scenario: GridWorld\n",
    "States: each grid cell (e.g., (0, 0), (1, 2))\n",
    "\n",
    "Actions: UP, DOWN, LEFT, RIGHT\n",
    "\n",
    "Transitions: move in direction unless at edge\n",
    "\n",
    "Rewards: green = +10 (goal), red = -5 (trap), others = 0\n",
    "\n",
    "No learning yet, just movement and reward tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c339163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State=(1, 0), Action=RIGHT, Reward=0, Discounted Total=0.00\n",
      "Step 2: State=(1, 1), Action=DOWN, Reward=-5, Discounted Total=-4.50\n",
      "Step 3: State=(1, 2), Action=DOWN, Reward=0, Discounted Total=-4.50\n",
      "Step 4: State=(1, 2), Action=DOWN, Reward=0, Discounted Total=-4.50\n",
      "Step 5: State=(1, 1), Action=UP, Reward=-5, Discounted Total=-7.78\n",
      "Step 6: State=(2, 1), Action=RIGHT, Reward=0, Discounted Total=-7.78\n",
      "Step 7: State=(2, 0), Action=UP, Reward=0, Discounted Total=-7.78\n",
      "Step 8: State=(1, 0), Action=LEFT, Reward=0, Discounted Total=-7.78\n",
      "Step 9: State=(1, 1), Action=DOWN, Reward=-5, Discounted Total=-9.93\n",
      "Step 10: State=(1, 0), Action=UP, Reward=0, Discounted Total=-9.93\n",
      "Step 11: State=(1, 0), Action=UP, Reward=0, Discounted Total=-9.93\n",
      "Step 12: State=(2, 0), Action=RIGHT, Reward=0, Discounted Total=-9.93\n",
      "Step 13: State=(1, 0), Action=LEFT, Reward=0, Discounted Total=-9.93\n",
      "Step 14: State=(0, 0), Action=LEFT, Reward=0, Discounted Total=-9.93\n",
      "Step 15: State=(0, 0), Action=UP, Reward=0, Discounted Total=-9.93\n",
      "Step 16: State=(0, 0), Action=UP, Reward=0, Discounted Total=-9.93\n",
      "Step 17: State=(0, 0), Action=UP, Reward=0, Discounted Total=-9.93\n",
      "Step 18: State=(0, 1), Action=DOWN, Reward=0, Discounted Total=-9.93\n",
      "Step 19: State=(1, 1), Action=RIGHT, Reward=-5, Discounted Total=-10.68\n",
      "Step 20: State=(0, 1), Action=LEFT, Reward=0, Discounted Total=-10.68\n",
      "Final Discounted Reward: -10.68\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "\n",
    "# ========== MDP Definition ==========\n",
    "GRID_SIZE = 3\n",
    "CELL_SIZE = 100\n",
    "WIDTH = HEIGHT = GRID_SIZE * CELL_SIZE\n",
    "\n",
    "# Rewards at specific states\n",
    "REWARDS = {\n",
    "    (2, 2): 10,   # Goal\n",
    "    (1, 1): -5    # Trap\n",
    "}\n",
    "\n",
    "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "GAMMA = 0.9  # Discount factor\n",
    "\n",
    "# ========== Pygame Setup ==========\n",
    "pygame.init()\n",
    "win = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"MDP - GridWorld\")\n",
    "\n",
    "font = pygame.font.SysFont(None, 36)\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 200, 0)\n",
    "RED = (200, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "agent_pos = [0, 0]  # Start state\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "# ========== Drawing Functions ==========\n",
    "\n",
    "def draw_grid():\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        for y in range(0, HEIGHT, CELL_SIZE):\n",
    "            rect = pygame.Rect(x, y, CELL_SIZE, CELL_SIZE)\n",
    "            pygame.draw.rect(win, BLACK, rect, 1)\n",
    "\n",
    "def draw_rewards():\n",
    "    for pos, reward in REWARDS.items():\n",
    "        x, y = pos\n",
    "        color = GREEN if reward > 0 else RED\n",
    "        pygame.draw.rect(win, color, (x*CELL_SIZE, y*CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "        label = font.render(str(reward), True, WHITE)\n",
    "        win.blit(label, (x*CELL_SIZE + 30, y*CELL_SIZE + 30))\n",
    "\n",
    "def draw_agent():\n",
    "    x, y = agent_pos\n",
    "    pygame.draw.circle(win, BLUE, (x*CELL_SIZE + CELL_SIZE//2, y*CELL_SIZE + CELL_SIZE//2), 20)\n",
    "\n",
    "# ========== MDP Transition Logic ==========\n",
    "\n",
    "def transition(state, action):\n",
    "    x, y = state\n",
    "    if action == 'UP' and y > 0:\n",
    "        y -= 1\n",
    "    elif action == 'DOWN' and y < GRID_SIZE - 1:\n",
    "        y += 1\n",
    "    elif action == 'LEFT' and x > 0:\n",
    "        x -= 1\n",
    "    elif action == 'RIGHT' and x < GRID_SIZE - 1:\n",
    "        x += 1\n",
    "    return (x, y)\n",
    "\n",
    "def move_agent():\n",
    "    global agent_pos, total_reward, step\n",
    "    action = random.choice(ACTIONS)  # For now, random policy\n",
    "    new_state = transition(agent_pos, action)\n",
    "    reward = REWARDS.get(new_state, 0)\n",
    "    discounted_reward = (GAMMA ** step) * reward\n",
    "    total_reward += discounted_reward\n",
    "    agent_pos[:] = new_state\n",
    "    step += 1\n",
    "\n",
    "    print(f\"Step {step}: State={tuple(agent_pos)}, Action={action}, Reward={reward}, Discounted Total={total_reward:.2f}\")\n",
    "\n",
    "# ========== Main Loop ==========\n",
    "\n",
    "def main():\n",
    "    clock = pygame.time.Clock()\n",
    "    run = True\n",
    "    max_steps = 20\n",
    "\n",
    "    while run and step < max_steps:\n",
    "        win.fill(WHITE)\n",
    "        draw_grid()\n",
    "        draw_rewards()\n",
    "        draw_agent()\n",
    "        pygame.display.flip()\n",
    "        pygame.time.delay(800)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "        move_agent()\n",
    "        clock.tick(60)\n",
    "        \n",
    "\n",
    "    print(\"Final Discounted Reward:\", round(total_reward, 2))\n",
    "    pygame.quit()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf315f",
   "metadata": {},
   "source": [
    "## RL Agent's Goal:\n",
    "The agent learns a policy that maximizes the cumulative reward over time."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3d55d548",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd076663",
   "metadata": {},
   "source": [
    "### Goal: Apply a Predefined Policy\n",
    "Instead of moving randomly, the agent will follow a fixed policy:\n",
    "→ Move RIGHT until end of row, then move DOWN to the next row, and repeat (like scanning a page).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de3fec",
   "metadata": {},
   "source": [
    "### Purpose:\n",
    "Help to understand how policy changes behavior and how trajectory + reward change compared to a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af1fbc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef simple_policy(state):\\n    x, y = state\\n    if x < GRID_SIZE - 1:\\n        return 'RIGHT'\\n    elif y < GRID_SIZE - 1:\\n        return 'DOWN'\\n    else:\\n        return None  # Terminal\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def simple_policy(state):\n",
    "    x, y = state\n",
    "    if x < GRID_SIZE - 1:\n",
    "        return 'RIGHT'\n",
    "    elif y < GRID_SIZE - 1:\n",
    "        return 'DOWN'\n",
    "    else:\n",
    "        return None  # Terminal\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55879558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e488b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State=(0, 0), Action=RIGHT, Next=(1, 0), Reward=0, Total=0.00\n",
      "Step 2: State=(1, 0), Action=RIGHT, Next=(2, 0), Reward=0, Total=0.00\n",
      "Step 3: State=(2, 0), Action=DOWN, Next=(2, 1), Reward=0, Total=0.00\n",
      "Step 4: State=(2, 1), Action=DOWN, Next=(2, 2), Reward=10, Total=7.29\n",
      "\n",
      "Final Discounted Reward: 7.29\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "\n",
    "# ========== MDP Definition ==========\n",
    "GRID_SIZE = 3\n",
    "CELL_SIZE = 100\n",
    "WIDTH = HEIGHT = GRID_SIZE * CELL_SIZE\n",
    "\n",
    "REWARDS = {\n",
    "    (2, 2): 10,   # Goal\n",
    "    (1, 1): -5    # Trap\n",
    "}\n",
    "\n",
    "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "GAMMA = 0.9  # Discount factor\n",
    "\n",
    "# ========== Pygame Setup ==========\n",
    "pygame.init()\n",
    "win = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"MDP - GridWorld (With Policy)\")\n",
    "\n",
    "font = pygame.font.SysFont(None, 36)\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "GREEN = (0, 200, 0)\n",
    "RED = (200, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "agent_pos = [0, 0]\n",
    "total_reward = 0\n",
    "step = 0\n",
    "\n",
    "# ========== Drawing Functions ==========\n",
    "\n",
    "def draw_grid():\n",
    "    for x in range(0, WIDTH, CELL_SIZE):\n",
    "        for y in range(0, HEIGHT, CELL_SIZE):\n",
    "            rect = pygame.Rect(x, y, CELL_SIZE, CELL_SIZE)\n",
    "            pygame.draw.rect(win, BLACK, rect, 1)\n",
    "\n",
    "def draw_rewards():\n",
    "    for pos, reward in REWARDS.items():\n",
    "        x, y = pos\n",
    "        color = GREEN if reward > 0 else RED\n",
    "        pygame.draw.rect(win, color, (x*CELL_SIZE, y*CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "        label = font.render(str(reward), True, WHITE)\n",
    "        win.blit(label, (x*CELL_SIZE + 30, y*CELL_SIZE + 30))\n",
    "\n",
    "def draw_agent():\n",
    "    x, y = agent_pos\n",
    "    pygame.draw.circle(win, BLUE, (x*CELL_SIZE + CELL_SIZE//2, y*CELL_SIZE + CELL_SIZE//2), 20)\n",
    "\n",
    "# ========== MDP Policy-Based Transition ==========\n",
    "\n",
    "def transition(state, action):\n",
    "    x, y = state\n",
    "    if action == 'UP' and y > 0: y -= 1\n",
    "    elif action == 'DOWN' and y < GRID_SIZE - 1: y += 1\n",
    "    elif action == 'LEFT' and x > 0: x -= 1\n",
    "    elif action == 'RIGHT' and x < GRID_SIZE - 1: x += 1\n",
    "    return (x, y)\n",
    "\n",
    "def simple_policy(state):\n",
    "    x, y = state\n",
    "    if x < GRID_SIZE - 1:\n",
    "        return 'RIGHT'\n",
    "    elif y < GRID_SIZE - 1:\n",
    "        return 'DOWN'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def move_agent_by_policy():\n",
    "    global agent_pos, total_reward, step\n",
    "\n",
    "    current_state = tuple(agent_pos)\n",
    "    action = simple_policy(current_state)\n",
    "\n",
    "    if action is None:\n",
    "        return False  # Terminal\n",
    "\n",
    "    next_state = transition(current_state, action)\n",
    "    reward = REWARDS.get(next_state, 0)\n",
    "    discounted_reward = (GAMMA ** step) * reward\n",
    "    total_reward += discounted_reward\n",
    "\n",
    "    agent_pos[:] = next_state\n",
    "    step += 1\n",
    "\n",
    "    print(f\"Step {step}: State={current_state}, Action={action}, Next={next_state}, Reward={reward}, Total={total_reward:.2f}\")\n",
    "    return True\n",
    "\n",
    "# ========== Main Loop ==========\n",
    "\n",
    "def main():\n",
    "    clock = pygame.time.Clock()\n",
    "    max_steps = 30  # to increase the no of steps\n",
    "    run = True\n",
    "\n",
    "    while run:\n",
    "        win.fill(WHITE)\n",
    "        draw_grid()\n",
    "        draw_rewards()\n",
    "        draw_agent()\n",
    "        pygame.display.flip()\n",
    "        pygame.time.delay(800)\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                run = False\n",
    "\n",
    "        keep_moving = move_agent_by_policy()\n",
    "        if not keep_moving:\n",
    "            break\n",
    "\n",
    "        clock.tick(60)\n",
    "\n",
    "    print(\"\\nFinal Discounted Reward:\", round(total_reward, 2))\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3155da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
